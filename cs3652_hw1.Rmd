---
title: "DSII_Homework1"
author: "Chirag Shah"
date: '2019-02-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(boot)
library(tidyverse)
library(glmnet)
library(ISLR)
library(pls)
```

a) Fit a linear model using least squares on the training data and calculate the mean square error using the test data. 

```{r}
train_data <- read_csv(file = "./solubility_train.csv") 
test_data <- read_csv(file = "./solubility_test.csv")
```

Creating a linear model with the training data with Solubility as the outcome. 

```{r}
linear_train <- lm(Solubility~., data = train_data)

summary(linear_train)
```


```{r}
pred <- predict(linear_train, newdata = test_data)

#Calculating Mean Squared Error on the test data
MSE = mean((test_data$Solubility - pred)^2)

print(MSE)
```

The mean squared error using the test data is 0.5558898. 

b) Fit a ridge regression model on the training data with lambda chosen by cross-validation. Report the test error. 

```{r}
#For ridge regression, creating an x matrix and y vector for train and test sets
xtrain <- model.matrix(Solubility~.,train_data)[,-1]
ytrain <- train_data$Solubility
xtest <- model.matrix(Solubility~.,test_data)[,-1]
ytest <- test_data$Solubility

#Using cross-validation to choose lambda (the tuning parameter)
set.seed(123)
cv.out = cv.glmnet(xtrain,ytrain,alpha = 0)
plot(cv.out)
```

```{r}
best_lambda = cv.out$lambda.min

#Ridge regression model creation
model = glmnet(xtrain, ytrain, alpha = 0,lambda = best_lambda)

#Printing out the model
model$beta
```

The lowest value of lambda, i.e. the best lambda, is 0.1478399

```{r}
#Fitting the above trainning model on test dataset 
pred2 = predict(model, s = best_lambda, newx = xtest)

#Calculating Accuracy via MSE
MSE2 = mean((pred2 - ytest)^2)

#Printing MSE
print(MSE2)
```

The mean squared error of the ridge regression using the cross validation approach is 0.51474. 

c) Fit a lasso model on the training data, with lambda chosed by cross-validation. Report the test error, along with the number of non-zero coefficient estimates. 

```{r}
#Using cross-validation to select lambda
set.seed(123)
cv.out3 = cv.glmnet(xtrain, ytrain, alpha = 1)
plot(cv.out3)
```

```{r}
best_lambda3 = cv.out$lambda.min

#lasso regression model using training data 
model3 = glmnet(xtrain, ytrain, alpha = 1, lambda = best_lambda3)

#Printing out the model
model3$beta
```

```{r}
#Fitting the above trainning model on the test dataset
pred3 = predict(model, s = best_lambda3, newx = xtest)

#Calculating Accuracy via MSE
MSE3 = mean((pred3 - ytest)^2)

#Printing MSE
print(MSE3)
```

The mean squared error using the lasso regression is 0.8191909

```{r}
#lasso coefficients
lasso_coef = predict(model3, type = "coefficients", s = best_lambda3)[1:length(model$beta),]

#non zero coefficients
lasso_coef[lasso_coef!= 0]
```

The number of non-zero coefficients including the intercept is 19. 

d) Fit a PCR model on the training data with M chosed by cross-validation. Report the test error, along with the value of M selected by cross-validation. 

```{r}
set.seed(123)

pcr.mod <- pcr(Solubility~., data = train_data, scale = TRUE, validation = "CV")

summary(pcr.mod)
```

```{r}
validationplot(pcr.mod, val.type = "MSEP", legendpos = "topright")
```

```{r}
#Fitting training model on the test dataset
pred4.pcr <- predict(pcr.mod, newdata = test_data, ncomp = 228)

#Calculating accuracy via MSE
MSE4 <- mean((pred4.pcr - ytest)^2)

#Printing MSE
print(MSE4)
```
The mean squared error of the pcr model is 0.555889.
The number of M's are 228. 

e) Briefly discuss the results obtained in (a)~(d)

Based on the results obtained, it seems that the ridge regression has the lowest mean squared error. Thus at it's face value, it seems that this model is the best in terms of describing the change in the outcome given the change in it's predictors. 