---
title: "DSII_Homework1"
author: "Chirag Shah"
date: '2019-02-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(boot)
library(tidyverse)
library(glmnet)
```

a) Fit a linear model using least squares on the training data and calculate the mean square error using the test data. 

```{r}
train_data <- read_csv(file = "./solubility_train.csv") 
test_data <- read_csv(file = "./solubility_test.csv")
```

Creating a linear model with the training data with Solubility as the outcome. 

```{r}
linear_train <- lm(Solubility~., data = train_data)

summary(linear_train)
```


```{r}
pred <- predict(linear_train, newdata = test_data)

#Calculating Mean Squared Error on the test data
MSE = mean((test_data$Solubility - pred)^2)

print(MSE)
```

The mean squared error using the test data is 0.5558898. 

b) Fit a ridge regression model on the training data with lambda chosen by cross-validation. Report the test error. 

```{r}
#For ridge regression, creating an x matrix and y vector for train and test sets
xtrain <- model.matrix(Solubility~.,train_data)[,-1]
ytrain <- train_data$Solubility
xtest <- model.matrix(Solubility~.,test_data)[,-1]
ytest <- test_data$Solubility

#Using cross-validation to choose lambda (the tuning parameter)
set.seed(123)
cv.out = cv.glmnet(xtrain,ytrain,alpha = 0)
plot(cv.out)
```

```{r}
best_lambda = cv.out$lambda.min

#Ridge regression model creation
model = glmnet(xtrain, ytrain, alpha = 0,lambda = best_lambda)

#Printing out the logistic model
model$beta
```

The lowest value of lambda, i.e. the best lambda, is 0.1478399

```{r}
#Fitting the above trainning model on test dataset 
pred2 = predict(model, s = best_lambda, newx = xtest)

#Calculating Accuracy via MSE
MSE = mean((pred2 - ytest)^2)

#Printing MSE
print(MSE)
```

The mean squared error of the ridge regression using the cross validation approach is 0.51474. 

c) Fit a lasso model on the training data, with lambda chosed by cross-validation. Report the test error, along with the number of non-zero coefficient estimates. 

```{r}
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet (xtrain,ytrain,alpha =1)
plot(cv.out)
```

```{r}
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
model =glmnet(xtrain,ytrain,alpha=1,lambda=bestlam)
#Printing out the logistic model
model$beta
```

```{r}
#Fitting trainning model on test set
pred=predict(model,s=bestlam ,newx=xtest)
#Calculating Accuracy
MSE=mean((pred-ytest)^2)
#Printing MSE
print(MSE)
```

```{r}
#Retrieving the lasso coefficients
lasscoef=predict(model,type="coefficients",s=bestlam)[1:length(model$beta),]
#Printing non zero coefficients
lasscoef[lasscoef!=0]
```

d) Fit a PCR model on the training data with M chosed by cross-validation. Report the test error, along with the value of M selected by cross-validation. 

e) Briefly discuss the results obtained in (a)~(d)